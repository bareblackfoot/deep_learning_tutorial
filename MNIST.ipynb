{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-1973dfd1d208>:8: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From c:\\users\\anuri\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From c:\\users\\anuri\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/mnist\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\users\\anuri\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/mnist\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\users\\anuri\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting data/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting data/mnist\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\users\\anuri\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "Packages loaded\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "tf.logging.set_verbosity(False)\n",
    "\n",
    "mnist = input_data.read_data_sets('data/mnist', one_hot=True)\n",
    "trainimg   = mnist.train.images\n",
    "trainlabel = mnist.train.labels\n",
    "testimg    = mnist.test.images\n",
    "testlabel  = mnist.test.labels\n",
    "print (\"Packages loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 784)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_img = mnist.train.images[5].reshape(28, 28)\n",
    "plt.imshow(sample_img).set_cmap('Greys')\n",
    "trainimg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainlabel.shape\n",
    "sample_label = mnist.train.labels[5]\n",
    "sample_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "_TENSORBOARD_PATH = \"./tensorboard/mnist/\"\n",
    "if not os.path.exists(_TENSORBOARD_PATH):\n",
    "    os.makedirs(_TENSORBOARD_PATH)    \n",
    "learning_rate   = 0.001\n",
    "training_epochs = 10\n",
    "batch_size      = 100\n",
    "display_step    = 2\n",
    "# Network Parameters\n",
    "n_input    = 784 # MNIST data input (img shape: 28*28)\n",
    "n_hidden_1 = 256 # 1st layer num features\n",
    "n_hidden_2 = 256 # 2nd layer num features\n",
    "n_hidden_3 = 256 # 3rd layer num features\n",
    "n_hidden_4 = 256 # 4th layer num features\n",
    "n_classes  = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# Store layers weight & bias\n",
    "stddev = 0.1\n",
    "with tf.variable_scope(\"MLP_WEIGHTS\"):\n",
    "    weights = {\n",
    "        'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1], stddev=stddev)),\n",
    "        'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2], stddev=stddev)),\n",
    "        'h3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3], stddev=stddev)),\n",
    "        'h4': tf.Variable(tf.random_normal([n_hidden_3, n_hidden_4], stddev=stddev)),\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden_4, n_classes], stddev=stddev))\n",
    "    }\n",
    "with tf.variable_scope(\"MLP_BIASES\"):\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_hidden_3])),\n",
    "        'b4': tf.Variable(tf.random_normal([n_hidden_4])),\n",
    "        'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network ready\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "def multilayer_perceptron(_X, _weights, _biases, _keep_prob):\n",
    "    with tf.variable_scope(\"MLP_LAYER_1\"):\n",
    "        layer_1 = tf.nn.dropout(tf.nn.relu(tf.add(tf.matmul(_X, _weights['h1'])\n",
    "                                                  , _biases['b1'])), _keep_prob)\n",
    "    with tf.variable_scope(\"MLP_LAYER_2\"):\n",
    "        layer_2 = tf.nn.dropout(tf.nn.relu(tf.add(tf.matmul(layer_1, _weights['h2'])\n",
    "                                                  , _biases['b2'])), _keep_prob)\n",
    "    with tf.variable_scope(\"MLP_LAYER_3\"):\n",
    "        layer_3 = tf.nn.dropout(tf.nn.relu(tf.add(tf.matmul(layer_2, _weights['h3'])\n",
    "                                                  , _biases['b3'])), _keep_prob) \n",
    "    with tf.variable_scope(\"MLP_LAYER_OUT\"):\n",
    "        layer_4 = tf.nn.dropout(tf.nn.relu(tf.add(tf.matmul(layer_3, _weights['h4'])\n",
    "                                                  , _biases['b4'])), _keep_prob) \n",
    "    return (tf.matmul(layer_4, _weights['out']) + _biases['out'])\n",
    "print (\"Network ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network Ready\n"
     ]
    }
   ],
   "source": [
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input], name=\"MLP_INPUT_x\")\n",
    "y = tf.placeholder(\"float\", [None, n_classes], name=\"MLP_TARGET_y\")\n",
    "keepratio = tf.placeholder(\"float\", name=\"MLP_DROPOUT\")\n",
    "\n",
    "# Construct model\n",
    "pred = multilayer_perceptron(x, weights, biases, keepratio)\n",
    "\n",
    "# Define loss and optimizer\n",
    "try:\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred, labels=y)) # Softmax loss\n",
    "except:\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y)) # Softmax loss\n",
    "optm = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) # Adam Optimizer\n",
    "\n",
    "# Accuracy \n",
    "corr = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accr = tf.reduce_mean(tf.cast(corr, \"float\"))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "print (\"Network Ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary ready\n"
     ]
    }
   ],
   "source": [
    "# Do some optimizations\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# Summary writer\n",
    "tf.summary.scalar('cross_entropy', cost)\n",
    "tf.summary.scalar('accuracy', accr)\n",
    "merged = tf.summary.merge_all()\n",
    "summary_writer = tf.summary.FileWriter(_TENSORBOARD_PATH, graph=sess.graph)\n",
    "print (\"Summary ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start!\n",
      "Epoch: 000/010 cost: 0.493168070\n",
      " Training accuracy: 0.920\n",
      " Test accuracy: 0.936\n",
      "Epoch: 002/010 cost: 0.111791424\n",
      " Training accuracy: 0.980\n",
      " Test accuracy: 0.967\n",
      "Epoch: 004/010 cost: 0.070520619\n",
      " Training accuracy: 0.990\n",
      " Test accuracy: 0.972\n",
      "Epoch: 006/010 cost: 0.050496400\n",
      " Training accuracy: 0.990\n",
      " Test accuracy: 0.978\n",
      "Epoch: 008/010 cost: 0.037076886\n",
      " Training accuracy: 0.990\n",
      " Test accuracy: 0.974\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "print (\"Start!\")\n",
    "# Training cycle\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0.\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    # Loop over all batches\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        # Fit training using batch data\n",
    "        summary, _ = sess.run([merged, optm], feed_dict={x: batch_xs, y: batch_ys, keepratio: 0.7})\n",
    "        # Compute average loss\n",
    "        avg_cost += sess.run(cost\n",
    "                , feed_dict={x: batch_xs, y: batch_ys, keepratio:1.})/total_batch\n",
    "        # Add summary\n",
    "        summary_writer.add_summary(summary, epoch*total_batch+i)\n",
    "    # Display logs per epoch step\n",
    "    if epoch % display_step == 0:\n",
    "        print (\"Epoch: %03d/%03d cost: %.9f\" % (epoch, training_epochs, avg_cost))\n",
    "        train_acc = sess.run(accr, feed_dict={x: batch_xs, y: batch_ys, keepratio:1.})\n",
    "        print (\" Training accuracy: %.3f\" % (train_acc))\n",
    "        test_acc = sess.run(accr, feed_dict={x: testimg, y: testlabel, keepratio:1.})\n",
    "        print (\" Test accuracy: %.3f\" % (test_acc))\n",
    "        \n",
    "print (\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run the command line**\n",
    "\n",
    "**tensorboard --logdir=\"./tensorboard\"**\n",
    "\n",
    "**Open http://localhost:6006/ into your web browser**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
