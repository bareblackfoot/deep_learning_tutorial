{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Tutorial <br> Day 4. CIFAR10 Classification\n",
    "\n",
    "Written by Nuri Kim.\n",
    "This code can be downloaded from [GitHub](https://github.com/bareblackfoot/deep_learning_tutorial).\n",
    "\n",
    "References of this notebook: \n",
    "*TensorFlow-Tutorials by [Magnus Erik Hvass Pedersen](http://www.hvass-labs.org/)* and \n",
    "*CIFAR-10 by [dhanushkamath](https://github.com/dhanushkamath/CIFAR-10/blob/master/Cifar10.ipynb)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This tutorial demonstrates the workflow of an image classification using TensorFlow with convolutional networks. Unlike MNIST dataset, CIFAR10 dataset has color images of 10 categories. Here, we define and optimize a simple mathematical model in TensorFlow. The results are then plotted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from time import time\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "try:\n",
    "    from urllib.request import urlretrieve\n",
    "except:\n",
    "    from urllib import urlretrieve\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys, os, math\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import tarfile\n",
    "import pickle\n",
    "\n",
    "print (\"Packages loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMS\n",
    "_BATCH_SIZE = 32\n",
    "_EPOCH = 10\n",
    "_SNAPSHOT_PREFIX = \"cifar10\"\n",
    "_NUM_VIS_EMBEDDING = 500\n",
    "_IMAGE_HEIGHT = 32\n",
    "_IMAGE_WIDTH = 32\n",
    "_IMAGE_CHANNELS = 3\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "_NUM_CLASSES = len(class_names)\n",
    "class_to_num = dict(zip(class_names, range(_NUM_CLASSES)))\n",
    "num_to_class = dict(zip(range(_NUM_CLASSES), class_names))\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was developed using Python 3.5 and TensorFlow version 1.10.1. Please check yours:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper-functions for loading images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions used to download dataset and pre-process images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_set(name=\"train\"):\n",
    "    x = y = None\n",
    "    maybe_download_and_extract()\n",
    "    folder_name = \"cifar10\"\n",
    "    f = open('./data/'+folder_name+'/batches.meta', 'rb')\n",
    "    f.close()\n",
    "\n",
    "    if name is \"train\":\n",
    "        for i in range(5):\n",
    "            f = open('./data/'+folder_name+'/data_batch_' + str(i + 1), 'rb')\n",
    "            try:\n",
    "                datadict = pickle.load(f, encoding='latin1') \n",
    "            except:\n",
    "                datadict = pickle.load(f)\n",
    "            f.close()\n",
    "\n",
    "            _X = datadict[\"data\"]\n",
    "            _Y = datadict['labels']\n",
    "\n",
    "            _X = np.array(_X, dtype=float) / 255.0\n",
    "            _X = _X.reshape([-1, _IMAGE_CHANNELS, _IMAGE_HEIGHT, _IMAGE_WIDTH])\n",
    "            _X = _X.transpose([0, 2, 3, 1])\n",
    "            _X = _X.reshape(-1, _IMAGE_HEIGHT*_IMAGE_WIDTH*_IMAGE_CHANNELS)\n",
    "\n",
    "            if x is None:\n",
    "                x = _X\n",
    "                y = _Y\n",
    "            else:\n",
    "                x = np.concatenate((x, _X), axis=0)\n",
    "                y = np.concatenate((y, _Y), axis=0)\n",
    "\n",
    "    elif name is \"test\":\n",
    "        f = open('./data/'+folder_name+'/test_batch', 'rb')\n",
    "        try:\n",
    "            datadict = pickle.load(f, encoding='latin1') \n",
    "        except:\n",
    "            datadict = pickle.load(f)\n",
    "        f.close()\n",
    "        x = datadict[\"data\"]\n",
    "        y = np.array(datadict['labels'])\n",
    "        x = np.array(x, dtype=float) / 255.0\n",
    "        x = x.reshape([-1, _IMAGE_CHANNELS, _IMAGE_HEIGHT, _IMAGE_WIDTH])\n",
    "        x = x.transpose([0, 2, 3, 1])\n",
    "        x = x.reshape(-1, _IMAGE_HEIGHT*_IMAGE_WIDTH*_IMAGE_CHANNELS)\n",
    "\n",
    "    return x, dense_to_one_hot(y)\n",
    "\n",
    "def dense_to_one_hot(labels_dense, num_classes=10):\n",
    "    num_labels = labels_dense.shape[0]\n",
    "    index_offset = np.arange(num_labels) * num_classes\n",
    "    labels_one_hot = np.zeros((num_labels, num_classes))\n",
    "    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "    return labels_one_hot\n",
    "\n",
    "def print_download_progress(count, block_size, total_size):\n",
    "    pct_complete = float(count * block_size) / total_size\n",
    "    msg = \"\\r- Download progress: {0:.1%}\".format(pct_complete)\n",
    "    sys.stdout.write(msg)\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "def maybe_download_and_extract():\n",
    "    main_directory = \"./data/\"\n",
    "    if not os.path.exists(main_directory):\n",
    "        os.makedirs(main_directory)\n",
    "    cifar_10_directory = main_directory+\"cifar10/\"\n",
    "    if not os.path.exists(cifar_10_directory):\n",
    "        url = \"http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
    "        filename = url.split('/')[-1]\n",
    "        file_path = os.path.join(main_directory, filename)\n",
    "        zip_cifar_10 = file_path\n",
    "        file_path, _ = urlretrieve(url=url, filename=file_path, \n",
    "                                   reporthook=print_download_progress)\n",
    "\n",
    "        print()\n",
    "        print(\"Download finished. Extracting files.\")\n",
    "        if file_path.endswith(\".zip\"):\n",
    "            zipfile.ZipFile(file=file_path, mode=\"r\").extractall(main_directory)\n",
    "        elif file_path.endswith((\".tar.gz\", \".tgz\")):\n",
    "            tarfile.open(name=file_path, mode=\"r:gz\").extractall(main_directory)\n",
    "        print(\"Done.\")\n",
    "\n",
    "        os.rename(main_directory+\"cifar-10-batches-py\", cifar_10_directory)\n",
    "        os.remove(zip_cifar_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "The size of CIFAR10 dataset is about  180MB and it will be downloaded automatically if it is not located in the given path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = get_data_set(\"train\")\n",
    "test_x, test_y = get_data_set(\"test\")\n",
    "\n",
    "print (\"Dataset loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CIFAR10 dataset has now been loaded and consists of 60000 32x32 color images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot a few images to see if data is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images():\n",
    "    fig, axes = plt.subplots(3, 3)\n",
    "    fig.subplots_adjust(hspace=0.6, wspace=0.3)\n",
    "    \n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # Plot image.\n",
    "        ax.imshow(train_x[i].reshape(_IMAGE_HEIGHT, _IMAGE_WIDTH, _IMAGE_CHANNELS))\n",
    "        \n",
    "        # Name of the true class.\n",
    "        cls_true_name = num_to_class[train_y[i].argmax()]\n",
    "        xlabel = \"class: {0}\".format(cls_true_name)\n",
    "        \n",
    "        # Show the classes as the label on the x-axis.\n",
    "        ax.set_xlabel(xlabel)\n",
    "        \n",
    "        # Remove ticks from the plot.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    return\n",
    "\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "show_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Graph\n",
    "\n",
    "The entire purpose of TensorFlow is to have a so-called computational graph that can be executed much more efficiently than if the same calculations were to be performed directly in Python. TensorFlow can be more efficient than NumPy because TensorFlow knows the entire computation graph that must be executed, while NumPy only knows the computation of a single mathematical operation at a time.\n",
    "\n",
    "TensorFlow can also automatically calculate the gradients that are needed to optimize the variables of the graph so as to make the model perform better. This is because the graph is a combination of simple mathematical expressions so the gradient of the entire graph can be calculated using the chain-rule for derivatives.\n",
    "\n",
    "TensorFlow can also take advantage of multi-core CPUs as well as GPUs.\n",
    "\n",
    "A TensorFlow graph consists of the following parts which will be detailed below:\n",
    "\n",
    "* Placeholder variables used to feed input into the graph.\n",
    "* Model variables that are going to be optimized so as to make the model perform better.\n",
    "* The model which is essentially just a mathematical function that calculates some output given the input in the placeholder variables and the model variables.\n",
    "* A loss measure that can be used to guide the optimization of the variables.\n",
    "* An optimization method which updates the variables of the model.\n",
    "\n",
    "In addition, the TensorFlow graph may also contain various debugging statements e.g. for logging data to be displayed using TensorBoard, which is not covered in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholder variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Placeholder variables serve as the input to the graph that we may change each time we execute the graph. We call this feeding the placeholder variables and it is demonstrated further below.\n",
    "\n",
    "First we define the placeholder variable for the input images. This allows us to change the images that are input to the TensorFlow graph. This is a so-called tensor, which just means that it is a multi-dimensional vector or matrix. The data-type is set to `float32` and the shape is set to `[None, _IMAGE_HEIGHT * _IMAGE_WIDTH * _IMAGE_CHANNEL]`, where `None` means that the tensor may hold an arbitrary number of images with each image being a vector of length `_IMAGE_HEIGHT * _IMAGE_WIDTH * _IMAGE_CHANNEL`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, _IMAGE_HEIGHT * _IMAGE_WIDTH * _IMAGE_CHANNELS], name='Input')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we have the placeholder variable for the true labels associated with the images that were input in the placeholder variable `x`. The shape of this placeholder variable is `[None, _NUM_CLASSES]` which means it may hold an arbitrary number of labels and each label is a vector of length `_NUM_CLASSES` which is 10 in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = tf.placeholder(tf.float32, shape=[None, _NUM_CLASSES], name='Output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we use a dropout regularization method for a fully connected layer, we have the placeholder variable for the probability to keep variables. If `keep_probability` is 1.0, all variables are the same without the dropout regularization. When `keep_probability` is 0.5, half of the variables in a fully connected layer become 0.0.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_probability = tf.placeholder(tf.float32, shape=[], name='keep_prob')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`global_step` saves how many optimization iterations are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = tf.Variable(initial_value=0, trainable=False, name='global_step')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow takes an image input in order of `[Batch, _IMAGE_HEIGHT, _IMAGE_WIDTH, _IMAGE_CHANNEL]`. We reshape the input image to make the tensorflow graph can understand the input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_image = tf.reshape(x, [-1, _IMAGE_HEIGHT, _IMAGE_WIDTH, _IMAGE_CHANNELS], name='images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network model  to be optimized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network consists of `4` convolutional networks and `2` fully connected layers. `conv1_1` layer consists of  filters with weight size `[3, 3, 3, 32]` and bias for each output channel, size `[32]`. `conv1_2` layer consists of filters with weight size `[3, 3, 32, 32]` and bias for each output channel, size `[32]`. `conv2_1` layer contains filters with size `[3, 3, 32, 64]` and bias for each output channel, size `[64]`. `conv3_1` layer consists of filters with size `[3, 3, 64, 128]` and bias for each output channel, size `[128]`. `fully_connected` layer has a filter with weight size `[_IMAGE_HEIGHT/8 * _IMAGE_WIDTH/8 * 128, 1000]`, and bias for each output channel, size 1000. Lastly, the `feat` layer has a filter with weight size `[1000, 10]` and bias for each output channel, size `[10]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### [Exercise 1] Change the CNN architecture to VGG16\n",
    "feat = slim.repeat(x_image, 2, slim.conv2d, 32, [3, 3], scope='conv1')\n",
    "feat = slim.max_pool2d(feat, [2, 2], padding='SAME', scope='pool1')\n",
    "feat = slim.repeat(feat, 1, slim.conv2d, 64, [3, 3], scope='conv2')\n",
    "feat = slim.max_pool2d(feat, [2, 2], padding='SAME', scope='pool2')\n",
    "feat = slim.repeat(feat, 1, slim.conv2d, 128, [3, 3], scope='conv3')\n",
    "feat = slim.max_pool2d(feat, [2, 2], padding='SAME', scope='pool3')\n",
    "feat = slim.flatten(feat, scope='flatten')\n",
    "feat = slim.fully_connected(feat, 1000, scope='fully_connected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### [Exercise 2] Add a regularization method\n",
    "feat = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = slim.fully_connected(feat, _NUM_CLASSES, scope='feat', activation_fn=None)\n",
    "softmax = tf.nn.softmax(y_pred)\n",
    "y_pred_cls = tf.argmax(softmax, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the variables in the network model which we defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.global_variables())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss-function to be optimized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the model better at classifying the input images, we must somehow change the variables for `weights` and `biases`. To do this we first need to know how well the model currently performs by comparing the predicted output of the model `y_pred` to the desired output `y_true`.\n",
    "\n",
    "The cross-entropy is a performance measure used in classification. The cross-entropy is a continuous function that is always positive and if the predicted output of the model exactly matches the desired output then the cross-entropy equals zero. The goal of optimization is therefore to minimize the cross-entropy so it gets as close to zero as possible by changing the `weights` and `biases` of the model.\n",
    "\n",
    "TensorFlow has a built-in function for calculating the cross-entropy. Note that it uses the values of the `logits` because it also calculates the softmax internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=y_pred, labels=y_true)\n",
    "except:\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=y_pred, labels=y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now calculated the cross-entropy for each of the image classifications so we have a measure of how well the model performs on each image individually. But in order to use the cross-entropy to guide the optimization of the model's variables we need a single scalar value, so we simply take the average of the cross-entropy for all the image classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a loss measure that must be minimized, we can then create an optimizer. In this case it is the basic form of Gradient Descent where the learning rate (step-size) is set to 0.001.\n",
    "\n",
    "Note that optimization is not performed at this point. In fact, nothing is calculated at all, we just add the optimizer-object to the TensorFlow graph for later execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_accuracy = 0\n",
    "epoch_start = 0\n",
    "\n",
    "### [Exercise 3] Write your own optimizer\n",
    "optimizer =  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a few more performance measures to display the progress to the user.\n",
    "\n",
    "This is a vector of booleans whether the predicted class equals the true class of each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(y_pred_cls, tf.argmax(y_true, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This calculates the classification accuracy by first type-casting the vector of booleans to floats, so that False becomes 0 and True becomes 1, and then calculating the average of these numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))*100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create TensorFlow session\n",
    "\n",
    "Once the TensorFlow graph has been created, we have to create a TensorFlow session which is used to execute the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize network parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variables for `weights` and `biases` in convolutional networks and fully connected layers must be initialized before we start optimizing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "print(\"Initialized variables.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper-functions to plot results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for plotting confusion matrix of test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    print('Drawing confusion matrix...')\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for plotting tsne examples of test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tsne(y_pred_val, y_true_val):\n",
    "    numbered = LabelEncoder().fit_transform(list(y_true_val))\n",
    "    c = plt.cm.get_cmap('hsv', len(set(numbered)))\n",
    "    colors=[]\n",
    "    for num in numbered:\n",
    "        temp=c(num)\n",
    "        colors.append(temp)\n",
    "        \n",
    "    print('Fitting TSNE...')\n",
    "    X_embedded = TSNE(n_components=2).fit_transform(y_pred_val[:10000])\n",
    "    X_embedded = np.array(X_embedded)\n",
    "    vis_x = X_embedded[:, 0]\n",
    "    vis_y = X_embedded[:, 1]\n",
    "    numbered = LabelEncoder().fit_transform(list(y_true_val))\n",
    "    c = plt.cm.get_cmap('hsv', len(set(numbered)))\n",
    "    plt.figure(num=None, figsize=(5, 5), dpi=80, facecolor='w', edgecolor='k')\n",
    "    plt.scatter(vis_x, vis_y, c=colors,alpha=1,linewidths=1,s=10, edgecolors='none')\n",
    "    plt.show()\n",
    "    return X_embedded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper-function to test the model at the middle of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(_global_step, epoch, y_pred_val, y_true_val):\n",
    "    global global_accuracy\n",
    "    global epoch_start\n",
    "\n",
    "    i = 0\n",
    "    predicted_class = np.zeros(shape=len(test_x), dtype=np.int)\n",
    "    while i < len(test_x):\n",
    "        j = min(i + _BATCH_SIZE, len(test_x))\n",
    "        batch_xs = test_x[i:j, :]\n",
    "        batch_ys = test_y[i:j, :]\n",
    "        predicted_class[i:j] = sess.run(\n",
    "            y_pred_cls,\n",
    "            feed_dict={x: batch_xs, y_true: batch_ys, keep_probability: 1.0}\n",
    "        )    \n",
    "        i = j\n",
    "    text_y_argmax = np.argmax(test_y, axis=1)  \n",
    "    correct = (text_y_argmax == predicted_class)\n",
    "    acc = correct.mean()*100\n",
    "    correct_numbers = correct.sum()\n",
    "\n",
    "    hours, rem = divmod(time() - epoch_start, 3600)\n",
    "    minutes, seconds = divmod(rem, 60)\n",
    "    mes = \"\\nEpoch {} - accuracy: {:.1f}% ({}/{}) - time: {:0>2}:{:0>2}:{:05.2f}\"\n",
    "    print(mes.format((epoch+1), acc, correct_numbers, len(test_x), int(hours), int(minutes), seconds))\n",
    "\n",
    "    if global_accuracy != 0 and global_accuracy < acc:\n",
    "        mes = \"This epoch receive better accuracy: {:.1f}% > {:.1f}%.\"\n",
    "        print(mes.format(acc, global_accuracy))\n",
    "        global_accuracy = acc\n",
    "\n",
    "    elif global_accuracy == 0:\n",
    "        global_accuracy = acc\n",
    "        \n",
    "    # Plot non-normalized confusion matrix\n",
    "    cnf_matrix = confusion_matrix(text_y_argmax, predicted_class)\n",
    "    np.set_printoptions(precision=2)\n",
    "    plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                          title='Confusion matrix, without normalization')\n",
    "    plot_tsne(y_pred_val, y_true_val)\n",
    "    \n",
    "    print(\"###########################################################################################################\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper-function to perform optimization iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for performing a number of optimization iterations so as to gradually improve the `weights` and `biases` of the model. In each iteration, a new batch of data is selected from the training-set and then TensorFlow executes the optimizer using those training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    global epoch_start\n",
    "    epoch_start = time()\n",
    "    batch_size = int(math.ceil(len(train_x) / _BATCH_SIZE))\n",
    "    i_global = 0\n",
    "\n",
    "    for s in range(batch_size):\n",
    "        batch_xs = train_x[s*_BATCH_SIZE: (s+1)*_BATCH_SIZE]\n",
    "        batch_ys = train_y[s*_BATCH_SIZE: (s+1)*_BATCH_SIZE]\n",
    "\n",
    "        start_time = time()\n",
    "        i_global, _, batch_loss, batch_acc = sess.run(\n",
    "            [global_step, optimizer, loss, accuracy],\n",
    "            feed_dict={x: batch_xs, y_true: batch_ys, keep_probability: 0.5})\n",
    "        duration = time() - start_time\n",
    "\n",
    "        if s % 100 == 0:\n",
    "            percentage = int(float(s)/float(batch_size)*100.0)\n",
    "            bar_len = 29\n",
    "            filled_len = int((bar_len*int(percentage))/100)\n",
    "            bar = '=' * filled_len + '>' + '-' * (bar_len - filled_len)\n",
    "\n",
    "            msg = \"Global step: {:>5} - [{}] {:>3}% - accuracy: {:.1f}% - loss: {:.4f} - {:.1f} sample/sec\"\n",
    "            print(msg.format(i_global, bar, percentage, batch_acc, batch_loss, _BATCH_SIZE / duration))\n",
    "    y_pred_val = sess.run(y_pred, feed_dict={x: train_x[:_NUM_VIS_EMBEDDING], keep_probability: 1.0})\n",
    "    y_true_val = train_y[:_NUM_VIS_EMBEDDING]\n",
    "    y_true_val = y_true_val.argmax(1)\n",
    "\n",
    "    test(i_global, epoch, y_pred_val, y_true_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weights are optimized and the accuracy on the test set increases as global step increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Start training\")\n",
    "train_start = time()\n",
    "\n",
    "for i in range(_EPOCH):\n",
    "    print(\"\\nEpoch: {}/{}\\n\".format((i+1), _EPOCH))\n",
    "    train(i)\n",
    "\n",
    "hours, rem = divmod(time() - train_start, 3600)\n",
    "minutes, seconds = divmod(rem, 60)\n",
    "mes = \"Best accuracy pre session: {:.2f}%, time: {:0>2}:{:0>2}:{:05.2f}\"\n",
    "print(mes.format(global_accuracy, int(hours), int(minutes), seconds))\n",
    "print(\"Done training\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
